/**
 * OpenRouter API Client
 * Communicates with OpenRouter API using Gemini 2.0 Flash (free tier).
 * Supports streaming responses for real-time README generation.
 * Includes automatic retry with exponential backoff for rate limits.
 */

import { getApiKey } from '../utils/secretStorage';

const OPENROUTER_API_URL = 'https://openrouter.ai/api/v1/chat/completions';

// Primary model and fallbacks (all free - more options = better availability)
const MODELS = [
    'google/gemini-2.0-flash-exp:free',           // Best quality, primary
    'google/gemma-2-9b-it:free',                   // Good quality fallback
    'meta-llama/llama-3.2-3b-instruct:free',      // Fast fallback
    'meta-llama/llama-3.1-8b-instruct:free',      // Another Llama option
    'mistralai/mistral-7b-instruct:free',         // Mistral fallback
    'microsoft/phi-3-mini-128k-instruct:free',    // Microsoft fallback
    'openchat/openchat-7b:free',                   // OpenChat fallback
    'huggingfaceh4/zephyr-7b-beta:free'           // Zephyr fallback
];

let currentModelIndex = 0;

function getCurrentModel(): string {
    return MODELS[currentModelIndex];
}

function switchToNextModel(): boolean {
    if (currentModelIndex < MODELS.length - 1) {
        currentModelIndex++;
        return true;
    }
    return false;
}

function resetToFirstModel(): void {
    currentModelIndex = 0;
}

// Retry configuration
const MAX_RETRIES = 3;
const INITIAL_DELAY_MS = 2000; // 2 seconds
const MAX_DELAY_MS = 30000; // 30 seconds

export interface ChatMessage {
    role: 'system' | 'user' | 'assistant';
    content: string;
}

export interface StreamCallback {
    onToken: (token: string) => void;
    onComplete: (fullContent: string) => void;
    onError: (error: Error) => void;
    onRetry?: (attempt: number, delayMs: number) => void;
}

/**
 * Sleep for a given number of milliseconds
 */
function sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
}

/**
 * Calculate delay with exponential backoff
 */
function getRetryDelay(attempt: number): number {
    const delay = INITIAL_DELAY_MS * Math.pow(2, attempt);
    return Math.min(delay, MAX_DELAY_MS);
}

/**
 * Make API request with retry logic
 */
async function fetchWithRetry(
    url: string,
    options: RequestInit,
    maxRetries: number = MAX_RETRIES,
    onRetry?: (attempt: number, delayMs: number) => void
): Promise<Response> {
    let lastError: Error | null = null;
    
    for (let attempt = 0; attempt <= maxRetries; attempt++) {
        try {
            const response = await fetch(url, options);
            
            // If rate limited (429), retry after delay
            if (response.status === 429 && attempt < maxRetries) {
                const delayMs = getRetryDelay(attempt);
                
                if (onRetry) {
                    onRetry(attempt + 1, delayMs);
                }
                
                await sleep(delayMs);
                continue;
            }
            
            return response;
        } catch (error) {
            lastError = error instanceof Error ? error : new Error(String(error));
            
            if (attempt < maxRetries) {
                const delayMs = getRetryDelay(attempt);
                
                if (onRetry) {
                    onRetry(attempt + 1, delayMs);
                }
                
                await sleep(delayMs);
            }
        }
    }
    
    throw lastError || new Error('Request failed after retries');
}

/**
 * Generate README content using OpenRouter API with streaming
 * Automatically tries fallback models if the primary model is busy
 */
export async function generateWithStreaming(
    messages: ChatMessage[],
    callbacks: StreamCallback
): Promise<void> {
    const apiKey = await getApiKey();
    
    if (!apiKey) {
        callbacks.onError(new Error('API key not configured. Please set your OpenRouter API key.'));
        return;
    }

    // Reset to first model at the start of each generation
    resetToFirstModel();
    
    // Try each model in sequence
    while (true) {
        const currentModel = getCurrentModel();
        const modelName = currentModel.split('/')[1]?.split(':')[0] || currentModel;
        
        try {
            callbacks.onToken(`ü§ñ Using model: **${modelName}**\n\n`);
            
            const response = await fetch(OPENROUTER_API_URL, {
                method: 'POST',
                headers: {
                    'Authorization': `Bearer ${apiKey}`,
                    'Content-Type': 'application/json',
                    'HTTP-Referer': 'https://github.com/ai-readme-generator',
                    'X-Title': 'AI README Generator - VS Code Extension'
                },
                body: JSON.stringify({
                    model: currentModel,
                    messages: messages,
                    stream: true,
                    temperature: 0.7,
                    max_tokens: 4096
                })
            });

            // Check for errors
            if (!response.ok) {
                const errorText = await response.text();
                
                // Rate limit (429) or model busy - try next model
                if (response.status === 429 || errorText.includes('busy') || errorText.includes('overloaded')) {
                    if (switchToNextModel()) {
                        const nextModel = getCurrentModel().split('/')[1]?.split(':')[0] || getCurrentModel();
                        callbacks.onToken(`\n‚ö†Ô∏è ${modelName} is busy. Switching to **${nextModel}**...\n\n`);
                        await sleep(1500); // Brief pause before trying next model
                        continue; // Try next model
                    } else {
                        // All models exhausted
                        resetToFirstModel();
                        callbacks.onError(new Error(
                            '‚è≥ All free models are currently busy.\n\n' +
                            'Please wait 1-2 minutes and try again.\n' +
                            'Tip: Free tier has shared rate limits across all users.'
                        ));
                        return;
                    }
                }
                
                // Model not found (404) - try next model
                if (response.status === 404) {
                    if (switchToNextModel()) {
                        const nextModel = getCurrentModel().split('/')[1]?.split(':')[0] || getCurrentModel();
                        callbacks.onToken(`\n‚ö†Ô∏è ${modelName} unavailable. Trying **${nextModel}**...\n\n`);
                        await sleep(500);
                        continue;
                    }
                }
                
                // Auth error
                if (response.status === 401) {
                    callbacks.onError(new Error('‚ùå Invalid API key. Please check your OpenRouter API key in settings.'));
                    return;
                }
                
                // Other error
                callbacks.onError(new Error(`API Error (${response.status}): ${errorText}`));
                return;
            }

            if (!response.body) {
                callbacks.onError(new Error('No response body received'));
                return;
            }

            // Successfully connected - stream the response
            const reader = response.body.getReader();
            const decoder = new TextDecoder();
            let fullContent = '';
            let buffer = '';
            let hasReceivedContent = false;

            while (true) {
                const { done, value } = await reader.read();
                
                if (done) {
                    break;
                }

                buffer += decoder.decode(value, { stream: true });
                const lines = buffer.split('\n');
                buffer = lines.pop() || '';

                for (const line of lines) {
                    if (line.startsWith('data: ')) {
                        const data = line.slice(6);
                        
                        if (data === '[DONE]') {
                            continue;
                        }

                        try {
                            const json = JSON.parse(data);
                            const token = json.choices?.[0]?.delta?.content;
                            
                            if (token) {
                                if (!hasReceivedContent) {
                                    // Clear the "Using model" message once content starts
                                    hasReceivedContent = true;
                                }
                                fullContent += token;
                                callbacks.onToken(token);
                            }
                            
                            // Check for error in response
                            if (json.error) {
                                throw new Error(json.error.message || 'API returned an error');
                            }
                        } catch (e) {
                            // Skip malformed JSON chunks
                        }
                    }
                }
            }

            // Check if we got any content
            if (fullContent.trim().length === 0) {
                if (switchToNextModel()) {
                    const nextModel = getCurrentModel().split('/')[1]?.split(':')[0] || getCurrentModel();
                    callbacks.onToken(`\n‚ö†Ô∏è No response from ${modelName}. Trying **${nextModel}**...\n\n`);
                    await sleep(500);
                    continue;
                }
            }

            callbacks.onComplete(fullContent);
            return; // Success!
            
        } catch (error) {
            // Network error - try next model
            if (switchToNextModel()) {
                const nextModel = getCurrentModel().split('/')[1]?.split(':')[0] || getCurrentModel();
                callbacks.onToken(`\n‚ö†Ô∏è Connection error. Trying **${nextModel}**...\n\n`);
                await sleep(1000);
                continue;
            }
            
            callbacks.onError(error instanceof Error ? error : new Error(String(error)));
            return;
        }
    }
}

/**
 * Generate README content without streaming (fallback)
 */
export async function generateNonStreaming(messages: ChatMessage[]): Promise<string> {
    const apiKey = await getApiKey();
    
    if (!apiKey) {
        throw new Error('API key not configured. Please set your OpenRouter API key.');
    }

    const response = await fetchWithRetry(
        OPENROUTER_API_URL,
        {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${apiKey}`,
                'Content-Type': 'application/json',
                'HTTP-Referer': 'https://github.com/ai-readme-generator',
                'X-Title': 'AI README Generator - VS Code Extension'
            },
            body: JSON.stringify({
                model: getCurrentModel(),
                messages: messages,
                stream: false,
                temperature: 0.7,
                max_tokens: 4096
            })
        }
    );

    if (!response.ok) {
        const errorText = await response.text();
        if (response.status === 429) {
            throw new Error('‚è≥ Rate limit reached. Please wait 30-60 seconds and try again.');
        }
        throw new Error(`API Error (${response.status}): ${errorText}`);
    }

    const json = await response.json() as { choices?: { message?: { content?: string } }[] };
    return json.choices?.[0]?.message?.content || '';
}

/**
 * Test API connection with a simple request
 */
export async function testConnection(): Promise<boolean> {
    try {
        const apiKey = await getApiKey();
        
        if (!apiKey) {
            return false;
        }

        const response = await fetch(OPENROUTER_API_URL, {
            method: 'POST',
            headers: {
                'Authorization': `Bearer ${apiKey}`,
                'Content-Type': 'application/json',
                'HTTP-Referer': 'https://github.com/ai-readme-generator',
                'X-Title': 'AI README Generator - VS Code Extension'
            },
            body: JSON.stringify({
                model: getCurrentModel(),
                messages: [{ role: 'user', content: 'Hi' }],
                max_tokens: 5
            })
        });

        // 429 is rate limit, but key is still valid
        return response.ok || response.status === 429;
    } catch {
        return false;
    }
}

/**
 * Get rate limit info
 */
export function getRateLimitInfo(): string {
    return `
üìä OpenRouter Free Tier Limits:
‚Ä¢ Model: Gemini 2.0 Flash Experimental (free)
‚Ä¢ Rate: ~20 requests/minute (shared)
‚Ä¢ Daily: ~200 requests/day
‚Ä¢ Note: Free models share capacity across all users

üí° Tips:
‚Ä¢ Wait 30-60 seconds if rate limited
‚Ä¢ Avoid rapid repeated requests
‚Ä¢ The extension auto-retries up to 3 times
    `.trim();
}
